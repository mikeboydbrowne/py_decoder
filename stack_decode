#!/usr/bin/env python
import optparse
import sys
import models
import copy
from collections import namedtuple

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=1, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=1, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

# TM contains tuples of words
tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

def extract_english(h):
  return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)

# tm should translate unknown words as-is with probability 1
for word in set(sum(french,())):
  if (word,) not in tm:
    tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write("Decoding %s...\n" % (opts.input,))

for french_sentence in french:
  # The following code implements a monotone decoding
  # algorithm (one that doesn't permute the target phrases).
  # Hence all hypotheses in stacks[i] represent translations of
  # the first i words of the input sentence. You should generalize
  # this so that they can represent translations of *any* i words.

  future_cost = {}

  # calculating future cost
  for length in range(1, len(french_sentence) + 1, 1):
    for start in range(0, len(french_sentence) + 2 - length, 1):
      end = start + length
      future_cost[(start,end)] = float("inf")
      if french_sentence[start:end] in tm:

        # getting future cost
        phrase_fcost = 0
        for phrase in tm[french_sentence[start:end]]:
          # adding tm cost
          phrase_fcost = phrase.logprob

          # adding lm cost
          lm_state_holder = lm.begin()
          for word in phrase.english.split():
            (lm_state_holder, word_logprob) = lm.score(lm_state_holder, word)
            phrase_fcost += word_logprob

          # comparing to current cost
          if abs(phrase_fcost) < abs(future_cost[start, end]):
            min_cost = phrase_fcost

      for i in range(start, end, 1):
        if french_sentence[start : i] in tm and french_sentence[i + 1 : end] in tm:
          min_cost_first  = float('inf')
          min_cost_last   = float('inf')
          firstphrase_fcost = 0
          lastphrase_fcost = 0

          for phrase in tm[french_sentence[start : i]]:
            # getting tm cost
            firstphrase_fcost = phrase.logprob

            # getting lm cost
            lm_state_holder = lm.begin()
            for word in phrase.english.split():
              (lm_state_holder, word_logprob) = lm.score(lm_state_holder, word)
              firstphrase_fcost += word_logprob

            if abs(firstphrase_fcost) < abs(min_cost_first):
              min_cost_first = firstphrase_fcost

          for phrase in tm[french_sentence[i + 1 : end]]:
            # getting tm cost
            lastphrase_fcost = phrase.logprob

            # getting lm cost
            lm_state_holder = lm.begin()
            for word in phrase.english.split():
              (lm_state_holder, word_logprob) = lm.score(lm_state_holder, word)
              lastphrase_fcost += word_logprob

            if abs(lastphrase_fcost) < abs(min_cost_first):
              min_cost_last = lastphrase_fcost

          if min_cost_first + min_cost_last < future_cost[(start,end)]:
            future_cost[(start,end)] = min_cost_first + min_cost_last

  # create named tuple so its easier to deal with the values we are working on
  hypothesis = namedtuple("hypothesis", "logprob, lm_state, predecessor, phrase, prev_translated, futureCost")
  initial_hypothesis = hypothesis(0.0, lm.begin(), None, None, [0 for _ in range(len(french_sentence) - 1, -1, -1)], 0.0)

  # initialize an array of dictionaries of size N+1 (where N is the number of tokens)
  stacks = [{} for _ in french_sentence] + [{}]

  # add a sentence start token as the initial hypothesis to start with
  stacks[0][lm.begin()] = initial_hypothesis

  # loop through all but the last stack in the array of stacks (so for each word)
  for i, stack in enumerate(stacks[:-1]):

    # loop through stack dictionary contents, starting with the values with the lowest log probability
    for current_hypothesis in sorted(stack.itervalues(),key=lambda h: -(h.logprob  + h.futureCost))[:opts.s]: # prune

      # getting the array of previously untranslated positions
      translated    = current_hypothesis.prev_translated
      untranslated  = []

      for l, k in enumerate(translated):
        if k == 0:
          untranslated.append(l)

      # getting the phrase groupings
      group = []
      phrases = []
      prev_index = 0
      first_time = False
      for n in untranslated:
        if not first_time:
          prev_index = n
          group.append(n)
          first_time = True
        else:
          if prev_index + 1 == n:
            group.append(n)
            prev_index = n
          else:
            phrases.append(group)
            first_time = False
      phrases.append(group)

      # running through the phrase groupings
      for subphrase in phrases:
        for p in subphrase:
          for q in subphrase:
            if p <= q:

              # if the current range of words exists in our translation model
              if french_sentence[p:q + 1] in tm:

                # not really looping over phrases here in the k=1 case, this line is akin to phrase = tm[french_sentence[i:j]]
                for phrase in tm[french_sentence[p:q + 1]]:
                  # add the logprob for this phrase to the logprob of the current hypothesis
                  logprob = current_hypothesis.logprob + phrase.logprob

                  # extract the current state of the language model
                  current_lm_state = current_hypothesis.lm_state
                  new_translated = copy.deepcopy(translated)

                  # find the log prob of each word in the phrase, given the current language models state
                  # then add the logprob into the logprob tally for this phrase
                  for word in phrase.english.split():
                    (current_lm_state, word_logprob) = lm.score(current_lm_state, word)
                    logprob += word_logprob

                  logprob += lm.end(current_lm_state) if q + 1 == len(french_sentence) else 0.0

                  # Adding the words that were translated
                  for g in range(p, q + 1, 1):
                    new_translated[g] = 1

                  new_futureCost = future_cost[(p, q+1)]

                  # create a new hypothesis value given the current set of data
                  new_hypothesis = hypothesis(logprob, current_lm_state, current_hypothesis, phrase, new_translated, new_futureCost)

                  total_translated = 0
                  for i in new_translated:
                    total_translated = total_translated + i

                  # add it to the current stack for the state if that state's stack is empty, or if the log prob is lower
                  if current_lm_state not in stacks[total_translated] or (stacks[total_translated][current_lm_state].logprob + stacks[total_translated][current_lm_state].futureCost) < (logprob + new_futureCost): # second case is recombination
                    stacks[total_translated][current_lm_state] = new_hypothesis
  # # take the lowest absolute log prob value as our winner
  winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)

  print extract_english(winner)
  # such verbose
  if opts.verbose:
    def extract_tm_logprob(h):
      return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
    tm_logprob = extract_tm_logprob(winner)
    sys.stderr.write("LM = %f, TM = %f, Total = %f\n" %
      (winner.logprob - tm_logprob, tm_logprob, winner.logprob))

  # for stack in stacks:
