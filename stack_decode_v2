#!/usr/bin/env python
import optparse
import sys
import models
import copy
from collections import namedtuple

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=1, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=1, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")
opts = optparser.parse_args()[0]

# TM contains tuples of words
tm = models.TM(opts.tm, opts.k)
lm = models.LM(opts.lm)
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

def extract_english(h):
  return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)

# tm should translate unknown words as-is with probability 1
for word in set(sum(french,())):
  if (word,) not in tm:
    tm[(word,)] = [models.phrase(word, 0.0)]

sys.stderr.write("Decoding %s...\n" % (opts.input,))
for french_sentence in french:
  # The following code implements a monotone decoding
  # algorithm (one that doesn't permute the target phrases).
  # Hence all hypotheses in stacks[i] represent translations of
  # the first i words of the input sentence. You should generalize
  # this so that they can represent translations of *any* i words.

  # create named tuple so its easier to deal with the values we are working on
  hypothesis = namedtuple("hypothesis", "logprob, lm_state, predecessor, phrase, prev_translated, order_trans")
  initial_hypothesis = hypothesis(0.0, lm.begin(), None, None, [0 for _ in range(len(french_sentence) - 1, -1, -1)], [])

  # print len(french_sentence)

  # initialize an array of dictionaries of size N+1 (where N is the number of tokens)
  stacks = [{} for _ in french_sentence] + [{}]

  # add a sentence start token as the initial hypothesis to start with
  stacks[0][lm.begin()] = initial_hypothesis

  # loop through all but the last stack in the array of stacks (so for each word)
  for i, stack in enumerate(stacks[:-1]):

    # print 'Iterating through stack #' + str(i)

    # loop through stack dictionary contents, starting with the values with the lowest log probability
    for current_hypothesis in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:opts.s]: # prune

      # print current_hypothesis

      # getting the array of previously untranslated positions
      translated    = current_hypothesis.prev_translated
      untranslated  = []

      for l, k in enumerate(translated):
        # print l
        # print k
        if k == 0:
          untranslated.append(l)

      # print 'Untranslated tokens: ' + str(untranslated)

      # getting the phrase groupings
      group = []
      phrases = []
      prev_index = 0
      first_time = False
      for n in untranslated:
        if not first_time:
          prev_index = n
          group.append(n)
          first_time = True
        else:
          if prev_index + 1 == n:
            group.append(n)
            prev_index = n
          else:
            phrases.append(group)
            first_time = False

      # group.append(len(french_sentence))
      phrases.append(group)

      # print phrases

      for subphrase in phrases:
        for p in subphrase:
          for q in subphrase:
            # print 'p = ' + str(p)
            # print 'q = ' + str(q)
            if p <= q:

              print 'Trying to translate: ' + str(french_sentence[p:q+1])
              # print 'Sample find' + str(french_sentence[0:1])

              # if the current range of words exists in our translation model
              if french_sentence[p:q + 1] in tm:

                # not really looping over phrases here in the k=1 case, this line is akin to phrase = tm[french_sentence[i:j]]
                for phrase in tm[french_sentence[p:q + 1]]:

                  print 'Phrase: ' + str(phrase)

                  # add the logprob for this phrase to the logprob of the current hypothesis
                  logprob = current_hypothesis.logprob + phrase.logprob

                  # extract the current state of the language model
                  current_lm_state = current_hypothesis.lm_state

                  translation_order = copy.deepcopy(current_hypothesis.order_trans)

                  # print current_lm_state

                  new_translated = copy.deepcopy(translated)

                  # find the log prob of each word in the phrase, given the current language models state
                  # then add the logprob into the logprob tally for this phrase
                  for word in phrase.english.split():

                    # print 'Word: ' + str(word)
                    # print 'Inputs:' + str((current_lm_state, word))
                    # print 'Outputs: ' + str(lm.score(current_lm_state, word))
                    (current_lm_state, word_logprob) = lm.score(current_lm_state, word)
                    logprob += word_logprob
                    # new_translated.append(p + i)

                  # add the log prob that this is the end of the sentence (once we hit the end)
                  logprob += lm.end(current_lm_state) if q == len(french_sentence) else 0.0

                  # print 'Previously translated words: ' + str(translated)

                  new_translated = copy.deepcopy(translated)

                  for g in range(p, q + 1, 1):
                    # print g
                    new_translated[g] = 1
                    translation_order.append(g)

                  print translation_order

                  # print 'Newly translated words: ' + str(new_translated)

                  # print new_translated

                  # print phrase
                  # print current_hypothesis

                  # create a new hypothesis value given the current set of data
                  new_hypothesis = hypothesis(logprob, current_lm_state, current_hypothesis, phrase, new_translated, translation_order)


                  words_translated = q - p + 1  # of words translated in this iteration
                  total_translated = 0          # getting the total # of words translated

                  for i in new_translated:
                    total_translated = total_translated + i

                  # print total_translated

                  # print 'Total tokens translated: ' + str(total_translated)

                  # for i in current_hypothesis.prev_translated
                  #   if current_hypothesis.prev_translated

                  # add it to the current stack for the state if that state's stack is empty, or if the log prob is lower
                  if current_lm_state not in stacks[total_translated] or stacks[total_translated][current_lm_state].logprob < logprob: # second case is recombination
                    stacks[total_translated][current_lm_state] = new_hypothesis

              # else:
                # print 'Phrase not found in translation model'

    # if total_translated == 6:
    #   print 'Current guess' + str(current_hypothesis) + '\n'

  # take the lowest absolute log prob value as our winner
  winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)

  # print extract_english(winner).split()
  # print stacks[-1]

###########
  # converting the translation back in order
  order = winner.order_trans

  # print order
  # # print len(extract_english(winner).split(" "))

  copy_trans = []
  new_trans = ["" for _ in extract_english(winner).split()]

  for z in winner.order_trans:
    if z < len(extract_english(winner).split(" ")) - 1:
      copy_trans.append(z)
  # print copy_trans

  words_arr = extract_english(winner).split()

  # print words_arr

  for i, y in enumerate(copy_trans):
    # print i
    # print y
    new_trans[y] = words_arr[i]

  # # # print extract_english(winner).split(" ")
  # print new_trans

  newtrans_str = ""
  for v, w in enumerate(new_trans):
    if v < len(new_trans):
      newtrans_str += w + " "
    else:
      newtrans_str += w

  print newtrans_str

##########

  # # def recombine_eng(a, b)
  # #   for c, d in enumerate a.split

  # final_trans = ['' for _ in extract_english(winner).split(" ")

  # # # for i, d in enumerate(extract_english(winner).split(" ")):
  # # #   print i
  # # #   print winner.order_trans[i]
  # # #   print d



  # print len(extract_english(winner).split(" "))
  # such verbose
  if opts.verbose:
    def extract_tm_logprob(h):
      return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
    tm_logprob = extract_tm_logprob(winner)
    sys.stderr.write("LM = %f, TM = %f, Total = %f\n" %
      (winner.logprob - tm_logprob, tm_logprob, winner.logprob))

# for i, stack in enumerate(stacks):
  # print 'Stack[' + str(13) + '] contains: ' + str(stacks[13]) + '\n\n\n'
